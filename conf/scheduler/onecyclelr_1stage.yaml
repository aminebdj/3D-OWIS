# @package _group_

scheduler:
  _target_: torch.optim.lr_scheduler.OneCycleLR
  max_lr: ${optimizer.lr}
  epochs: ${trainer.max_epochs}
  # need to set to number because of tensorboard logger
  steps_per_epoch: -1

pytorch_lightning_params:
  interval: step
